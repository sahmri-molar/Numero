---
title: "Numero Introduction"
author: 
- name: Song Gao
  affiliation:
  - &sahmri South Australian Health And Medical Research Institute, Adelaide, Australia
  - &uni University of Adelaide, School of Biological Sciences, Adelaide, Australia
  email: song.gao@sahmri.com  
- name: Stefan Mutter
  affiliation: 
  - *sahmri 
  - *uni
- name: Ville-Petteri MÃ¤kinen
  affiliation: 
  - *sahmri
  - *uni
  - Computational Medicine, Institute of Health Sciences, Faculty of Medicine, University of Oulu, Finland
  
date: "`r doc_date()`"
package: "`r pkg_ver('Numero')`"
output: BiocStyle::html_document2
bibliography: Numero.bib
csl: apa-old-doi-prefix.csl
abstract: >
  Numero is a multivariate statistical framework that creates a two dimensional map out of a multi-dimensional input. The framework uses self-organising maps. Its main purpose is to easily visualise and consequently support the identification of subgroups of samples/individuals. The framework is especially useful but not limited to data-driven discovery of epidemiologically, clinically and/or biologically relevant subtypes and subgroups. 
vignette: >
  %\VignetteIndexEntry{Numero Introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}  
---

<!--
%% \VignetteEngine{knitr::knitr}
-->
```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```
```{r, echo = FALSE, results='asis'}
BiocStyle::markdown()
BiocStyle::markdown(css.files = c('custom.css'))
```

# Prerequisites and Installation

`r Biocpkg('Numero')`  requires `r CRANpkg('Rcpp')` to compile the C++ code that is embedded in the library. To install `r Rpackage('Rcpp')`, use the command:

```{r, eval=FALSE}
install.packages( "Rcpp" )
```

A general overview of `r Rpackage('Rcpp')` can be accessed via:

```{r, eval=FALSE}
vignette('Rcpp-introduction')
vignette('Rcpp-FAQ')
```

After installing `r Rpackage('Rcpp')`, `r Rpackage('Numero')` can be easily installed either via Bioconductor (recommended)

```{r,eval=FALSE}
## try http:// if https:// URLs are not supported
source("https://bioconductor.org/biocLite.R")
biocLite("Numero")
```

or by downloding the compressed file package file from Bioconductor and using the following command:

```{r, eval=FALSE}
install.packages( "<download_dir>/<Numero file>.tar.gz", type="source", repos=NULL )
```

```{r setUpLibrary, eval=TRUE, include=FALSE}
library(Numero)
```

# Introduction

`r Biocpkg('Numero')` is a multivariate statistical framework that creates a two dimensional map out of a multi-dimensional input. The framework uses the self-organising map (SOM), a well-established unsupervised learning algorithm [@kohonen_som]. Previous versions of this software (written for MatLab) were successfully used on a range of metabolomics data sets for various diseases [@makinen_1h_2008; @makinen_metabolic_2008; @tukiainen_multi-metabolite_2008; @kumpula_characterization_2010; @bernardi_new_2010; @wurtz_characterization_2011; @makinen_metabolic_2012; @kuusisto_interplay_2012; @makinen_triglyceride-cholesterol_2013]. The main purpose of the framework is to easily visualise and consequently support the definition of subgroups of samples/individuals. The framework is especially useful but not limited to data-driven discovery of epidemiologically, clinically and/or biologically relevant subgroups.  

## How to use a self-organsing map (SOM)
The use of a SOM requires a staged approach: 
1. training the SOM, 
2. matching samples and 
3. visualising the results. 

In this vignette, we introduce an example where all samples are used for training, matching and visualisation, but we limit the training to some of the measured variables. The common example, that we re-produce here, is to train `r Rpackage('Numero')` with the biomarker data (e.g. serum measurements) and visualise (subgroups for) that data and additional data on disease (e.g. diabetes diagnosis) and data on survival. Therefore, the SOM reveals subgroups that are more vulnerable to disease and have fewer chances of survival. However, our framework is not limited to such a setup. Other splits of samples for training, matching and visualisation are possible but not part of this introduction. In general, `r Rpackage('Numero')` allows to use a subset of samples/individuals and/or a subset of variables for training, matching and visualisation. 

An introduction to the SOM and in particular circular SOM as used in this software can be found the in supplements (Supporting Information 1 (p.2-5)) from @makinen_metabolic_2012. Other introductions to the SOM (with rectangular maps) are in the supplements from @makinen_1h_2008 and @makinen_metabolic_2008.

## Districts on the map {#subtypesSubgroups}

The self-organising map (SOM) positions samples/individuals on a two-dimensional map so that samples/individuals that are similar are closer together on the two-dimensional map. Analagous to a city map, the self-organising map is divided into districts and samples/individuals that are the most similar will be found in the same district. 

The SOM has an important property: the complexity of the model does not depend on the number of variables. Furthermore, the SOM is a non-parametric method in the sense that it makes only mild assumptions about the dataset and it can detect non-linear patterns. For these reasons, the SOM works well as an exploratory complement to the standard medical, biological or epidemiological statistics toolbox.

In `r Rpackage('Numero')` each sample/individual is mapped to exactly one district on the map. The number of district is an (indirect) input parameter. Our maps are circular to avoid boundary effects of rectangular maps. Thus, the user specifies the radius of the map. The number of districts depends on the radius. 

Upon visual inspection of the maps, users can define subgroups by combining all samples/individuals from at least one district. Figure 1A shows a circular example map with 40 districts (created by setting the radius to 3). They are numbered from 1 to 40. Figure 1B shows the same map with subgroup boundaries. The subgroup under consideration is made up of the samples/individuals that were mapped to districts 2, 3, 12, 13, 14, 22, 23, 24, 39 and 40.

![[Figure 1. ](#fig1) Districts and subgroups](subtypesSubgroups.png)

# The `r Rpackage('Numero')` pipeline

`r Rpackage('Numero')` operates as a pipeline: loading the data, pre-processing the data, training the model, matching and visualisation. The pre-processing step is not an integral part of `r Rpackage('Numero')` as it can be done natively in R. However, we provide an example in the [pre-processing section](#pre). 

Figure 2 and Table 1 show the complete pipeline.

![[Figure 2. ](#fig2) The `r Rpackage('Numero')` pipeline](pipeline.png)

We will explain each function with the use of an example in the [next section](#howto). There is a link in the table that directly jumps to the corresponding explanation. The list in the table is exhaustive. These are all functions in the `r Rpackage('Numero')` package.

Function | Part of pipeline | Short description 
---------|-----------------------|----------------------
*nroMatrix*|[Load the data](#load)|Import a data matrix from a tab-delimited text file
*nroKmeans*|[Setup  and train the SOM](#setup)|Create the initial seed profile of the SOM
*nroKoho*|[Setup  and train the SOM](#setup)|Create the SOM
*nroTrain*|[Setup  and train the SOM](#setup)|Adapt the SOM to a set of multivariable data
*nroMatch*|[Match individuals/samples](#match)|Compares the multivariate samples to a SOM and determines their location
*nroPermute*|[Estimate map statistics](#stats)|Estimate the statistical significance for a regional pattern using permutations
*nroAggregate*|[Colourise maps and save figures](#colour)|Estimate regional aggregates based on assigned map locations
*nroColorize*|[Colourise maps and save figures](#colour)|Assign colors to subtypes
*nroLabel*|[Colourise maps and save figures](#colour)|Optimise the selection of labels for subtypes
*nroCircus*|[Colourise maps and save figures](#colour)|Create a Scalable Vector Graphics (SVG) code for a map
*nroFigure*|[Colourise maps and save figures](#colour)|Save a map to a Scalable Vector Graphics (SVG) file

Table: Table 1. The functions of the `r Rpackage('Numero')` pipeline.

# How to use the `r Rpackage('Numero')` pipeline {#howto}

This section gives a detailed example of how to do analysis using this package. `r Rpackage('Numero')` is designed as a pipeline and we will step through it from start to end.

## Example dataset
The example dataset is a subset of a publically available dataset [@makinen_1h_2008] on type 1 diabetes patients. Our version of the dataset contains `r nrow(nroMatrix(system.file("extdata", "finndiane_dataset.txt", package = "Numero"), keyvars = "INDEX"))` individuals and `r ncol(nroMatrix(system.file("extdata", "finndiane_dataset.txt", package = "Numero"), keyvars = "INDEX"))` variable columns. 

```{r showReadme, eval = TRUE, echo=FALSE, comment=''}
#read in README file
readmeFile <- system.file("extdata", "README.txt", package = "Numero")
myText <- readLines(readmeFile)

#only show the relevant portion of the README file, if it fails show all
myMin <- max(which(myText == 'Baseline data:'),0)
myMax <- min(which(myText == 'Missing values are shown as \'NaN\'.'),length(myText))
myText <- myText[myMin:myMax]

#display results
cat(myText, sep = '\n')
```

It contains five biomarkers measured at baseline that are important with regards to developing complications in type 1 diabetes such as diabetic kidney disease and cardiovascular diseases. These are the urinary albumin excretion rate and serum creatinine which are both biomrakers for kidney function. Serum triglycerides, total cholesterol and HDL2 cholesterol are biomarkers for macrovascular health. We also know age, sex and the diabetes duration and whether there have been already complications or co-morbidities (e.g. diabetic retinopathy, diabetic kidney disease, macrovascular disease or metabolic syndrome at baseline). At follow-up we know the survival of individuals.

The aim of our data-driven analysis is to find vulnerable subgroups in these individuals with type 1 diabetes. One of the advantages of `r Rpackage('Numero')` is that it can handle a large number of risk factors. For this example we limited ourselves to five biomarkers.

## Load the data {#load}
First we need to load the data into a matrix using the *nroMatrix* function. The data has to be stored in a tab-delimited text file. In our example, the *INDEX* column contains the patient identifier. By declaring it in *keyvars*, it will be automically dealt with in the analysis.
```{r setUpExample, eval=TRUE}
exampleFile <- system.file("extdata", "finndiane_dataset.txt", package = "Numero")
db <- nroMatrix(exampleFile, keyvars = "INDEX")
```

Second, we need to define our training set for the SOM. Our study design is to use the biomarkers from our data set and place individuals on the map according to similarity in these five biomarkers. The aim is to predict disease status and survival and thus identify vulnerable subgroups. Therefore, we will be using all individuals for training, matching and visualisation and the biomarkers for training and all variables for matching and visualisation. Consequently, we simply select a subset of our input matrix for training.

```{r preprocess1, eval=TRUE}
# Select training variables.
trvars <- c("uALB_log", "TG_log", "CHOL", "HDL2C", "CREAT_log")
trdata <- db[,trvars]
```


## Pre-processing {#pre}
Pre-processing the data is an important step. However, it is not an integral part of `r Rpackage('Numero')` as pre-processing support is already provided in *R*.

Here, we standardise the data to eliminate differences in measurement units. Otherwise, one biomarker might unduly dominate the results. We recommend standardisation prior to using our package. In addition, we adjust the data according to sex which is also a common pre-processing step (see @makinen_metabolic_2012 for example). When you conceive your own study, it is important to execute the appropriate pre-processing step in order to have valid results.
```{r preprocess2, eval=TRUE}
# Separate men and women.
men <- which(!(db[,"MALE"] == 0))
women <- which(db[,"MALE"] == 0)

# Standardise training data to reduce gender differences
# and to eliminate differences in measurement units.
trdata[men,] <- scale.default(trdata[men,])
trdata[women,] <- scale.default(trdata[women,])
```

## Setup  and train the SOM {#setup}

After pre-processing, it is time to initialise the SOM. In `r Rpackage('Numero')` we use the centroids of k-Means clusters on the training data. That way, the units in the SOM are initialised based on clusters in the data. The function *nroKmeans* provides this information. The user can choose the number of centroids. 

Then *nroKoho* initialises the SOM using the centroids and a user defined radius for the circular map. The radius controls the resolution of the SOM. Here, we choose 3 centroids and set the radius to 3. Figure 1A from the [district section](#subtypesSubgroups) shows a SOM of radius 3.

```{r setup1, eval=TRUE}
# Use K-means clustering to determine seed profiles for the SOM. 
km <- nroKmeans(x=trdata, k = 3)
sm <- nroKoho(seeds=km$centroids, radius = 3)
```

Training of the SOM is now straight-forward by inputting the initialised SOM and the training data into the *nroTrain* function.

```{r setup2, eval=TRUE}
# Train the SOM with the standardized data.
sm <- nroTrain(som=sm, x=trdata)
```

## Match individuals/samples {#match}

The next step after training is to match all individuals/samples to exactly one district. This best matching district is also referred to as best matching unit (BMU) in the code. It is found by the *nroMatch* function. The function takes the SOM and the data as input. It outputs a data frame with a row for each sample and three columns: BMU contains the best matching district number and two columns QUALITY and RDATA that can be used to assess the quality of the SOM. We discuss the two quality column in their own section on [SOM qauality](#qm) as they are not an integral part of the pipeline. For now, the column of interest is the best matching district one.

```{r match1, eval=TRUE}
# Find best-matching subtypes for all samples.
matches <- nroMatch(som=sm, x=trdata)
```

As the output below shows, the individual in the first row is matched to district `r head(matches$BMU)[1]`, the individual in the second row to district `r head(matches$BMU)[2]`, etc.

```{r match2, eval=TRUE}
# Show the first best-matching subtypes
head(matches$BMU)
```

## Estimate map statistics {#stats}

The next step in the pipeline is to estimate the significance of a SOM pattern e.g. whether significant differences between districts on the map exist. This is important as the SOM algorithm finds a pattern for "true" as well as random data. By definition p-values show the chance that the difference in the random patterns could be more extreme than what we observed from the "true" data.

Following this definition, we use permutations and the *nroPermute* function to calculate statistical significance. It takes the SOM, the best matching districts, the values of the variable under consideration and the maximum number of permutations (the default value is 10,000).

For each variable, *nroPermute*, performs the following procedure: In each permutation, it randomly assigns best matching districts to all samples/individuals and re-calculates the district values for this random assignment. The function then counts in how many permutations the standard deviations for random data are the same or more extreme than the standard deviation  for the "true" data. The resuting p-value for the variable under consideration is simply this count divided by the number of permutations.

To use the *nroPermute* function in your pipeline, simply loop over all variables.

```{r stats1, eval=TRUE}
# Determine statistics for all variables.
stats <- matrix(NA, ncol(db), 5)
rownames(stats) <- colnames(db)
for( vname in colnames(db) ) {
    
    # Check if a training variable.
    nsim <- NA
    pos <- match(vname, trvars)
    if(is.na(pos)) nsim <- 10000
 
    # Estimate dynamic range of variation.
    tmp <- nroPermute(map=sm, x=db[,vname], bmus=matches$BMU, n=nsim)
    colnames(stats) <- colnames(tmp)
    stats[vname,] <- as.matrix(tmp)
}
```

In this example, the output of *nroPermute* is saved in a data frame called stats. It contains five columns: P is a parametric estimate for statistical significance. FREQ is the frequency-based estimate for statistical significance, Z is the estimated z-score of how far the observed map plane is from the average randomly generated layout, NDATA indicates how many data values were used, and NSIM tells the number of completed permutations.

```{r stats2, eval=TRUE}
print(stats)
```

Note that since the training variables are directly responsible for the sample mapping, they all have  non-meaningful p-values, which cannot be used for results interpretation.

## Colourise maps and save figures {#colour}

As stated previously, the main purpose of the framework is to easily visualise and consequently support the definition of subgroups of samples/individuals. Therefore, `r pkg_ver('Numero')` offers visualisation support in its pipeline. Up to now, we know which variables form some significant subgroups but we do not easily know where these subgroups are on the map and how they overlap. This is where the colouring of the map comes in.

The first step in colouring is to to make it easy and consistent to differentiate the colourings of the map for variable A that has a significant subgroup pattern from variable B that has an insignificant pattern. This is done by defining one colour range for all variables that provides this information.

### Define the colour range for all SOMs

The significance (e.g. P-values and Z-scores) of the differences among subtypes for all variables is encoded into one dynamic colour range for all variable colourings of the SOM as the code below shows.

```{r colour1}
# Determine suitable dynamic color range across all variables.
# The middle point between the mean training variable score and
# the strongest evaluation variable usually works well.
tr <- which(is.na(stats[,"P"]))
ev <- which(stats[,"P"] >= 0.0)
zbase <- 0.5*(mean(stats[tr,"Z"], na.rm=TRUE)
              + max(stats[ev,"Z"], na.rm=TRUE))

# Make sure the base scale is large enough to prevent non-significant
# signals from getting bright colors.
zbase <- max(zbase, 2.0)

# Set color scale factors. Ideally, these should all be between
# 0 and 1. In practice, it is often better to adjust the scale
# so that the strongest evaluation variable gets a reasonable
# value (e.g. between 0.5 and 1.0), whereas the training variables
# can be over 1.
amplitudes <- stats[,"Z"]/zbase
```


### Define the actual colour for each subtype

After having defined the colour range for the SOM, we colour all districts of the map for each variable indepdendent of whether the variable has been used in training or testing. As you can see in the code below, we simply loop through all the variables: `r colnames(db)`.

Defining the actual colour for a district is a two step process: calculating the actual numeric value for the district and given the colour range determining where in the range this value lies. The first step is performed by *nroAggregate* and the second step by *nroColorize* for each variable

The *nroAggregate* takes the SOM *sm*, the values of the variable from the dataset and the best matching districts *BMU* and outputs a numeric estimate for each district. *nroColorize* chooses the colour given that district value and the colour range.

The function *nroLabel* places a few labels (the district values) on the map so that the map colours can be interpreted and put into context without cluttering of text. Without using *nroLabel*, the users would not know which colour correspond to which value.

In the penultimate step, we create the *svg* file with *nroCircus*. As you can see in the code below, we add a title and the p-value to the map output. This is recommended, as this information is essential when interpreting the map.

Lastly, *nroFigure* saves the *svg* content to a file.

```{r colour3, eval=FALSE}
# Process each variable separately. Note that to make the color scales
# comparable, all the statistics had to be estimated beforehand.
for( vname in colnames(db) ) {

     # Estimate subtype values.
     plane <- nroAggregate(map=sm, x=db[,vname], bmus=matches$BMU)
     
     # Determine subtype colors.
     clrs <- nroColorize(values=plane, amplitude=amplitudes[vname])

     # Determine which subtype labels should be shown.
     labs <- nroLabel(map=sm, values=plane)
     
     # Create a vector graphics object.
     pval <- stats[vname,"P"]
     ttxt <- vname
     if(is.na(pval) == FALSE) { # add p-value for evaluation variables
         ttxt <- sprintf("%s, P = %.2e", vname, pval);
     }
     smfig <- nroCircus(map=sm, colors=clrs, labels=labs, title=ttxt)
     
     # Save figure.
     fpath <- paste(vname, ".svg", sep="")
     nroFigure(file=fpath, scene=smfig)
}

```

One note regarding value estimation in *nroAggregate*: Because of matching, it is possible to train the SOM on standardised and adjusted values but to colour the SOM according to the true original data value. Consequently, the parameter *x* in the *nroAggregate* function is set to the original data vector and not the standardised and adjusted training vector. An example for this is HDL2 cholesterol in Figure 4C in the [results section](#visual). 

This is the end of the analysis pipeline. An example map for type 1 diabetes duration can be found in the next [section](#interpretation).

## Results and a possible subgroup interpretation {#interpretation}
Figure 3 shows the resulting SOM with the colouring for type 1 diabetes duration.

![[Figure 3. ](#fig3) SOM colouring for type 1 diabetes duration](T1D_DURAT.svg)

Above the map, it shows the name of the variable and its p-value. Here, districts with a shorter diabetes duration are all in the bottom left area of the map.

### Analysing subgroups visually {#visual}

The interesting part in the analysis is to compare different colourings and define subgroups. Thanks to the map colourings for each variable we can do this visually and this is a major advantage of the framework.

Figure 4 shows our example subgroup analysis.

![[Figure 4. ](#fig4) Subgroups in our type 1 diabetes examples](subgroupingExample.png)

Our subgrouping here is based on two colourings: the one for type 1 diabetes duration and the one for creatinine (Figure 4A). One possible subgrouping of districts is to divide the SOM into three subgroups. The aim of our analysis is to find vulnerable subgroups. Thus, here we focus how diabetes duration and creatinine levels influence vulnerability regarding disease and survival. 

The three subgroups are: One with a lower diabetes duration and lower creatinine (Subgroup *Shorter Duration*, see Figure 4A) on the left side, one with a longer diabetes duration and high creatinine (Subgroup *High Creatinine*) at the top of the map and one with a longer diabetes duration and low creatinine (Subgroup *Low Creatinine*) on the lower right. Note as creatinine levels were used in training, the map does not show a p-value. Now that the subgroups are defined and we know which districts belong to a subgroup, you can go back into the SOM output and calculate averages for each variable and subgroup.

We are now interested how our three subgroups do in terms of disease and survival at follow up (Figure 4B). We can see that Subgroup *High Creatinine* was likely to also have diabetic kidney disease. We can also see that a long diabetes duration has not necessarily led to kidney disease as Subgroup *Low Creatinine* had a longer duration of type 1 diabetes but also lower percentages of kidney disease. However, it is Subgroup *Shorter Duration* that was the least affected. Looking at survival, Subgroup *High Creatinine* showed the highest percentage of deceased individuals and the other two subgroups had much lower rates.

We can also compare our subgroups to other biomarkers. Figure 4C shows an example for cholesterol in high density lipoprotein 2 (HDL2). We can see that Subgroup *Low Creatinine* had high HDL2 cholesterol levels especially compared to Subgroup  *High Creatinine* which also had a higher mortality.

Figure 4D shows another nice property of having visual maps. As explained, the data was adjusted for sex prior to the analysis. A successful adjustment means that there are no patterns on the map colouring for sex. Looking at the SOM colouring for male, we cannot see any pattern and the p-value for the SOM is not significant at the 0.05 level either.

# Checking the quality of maps {#qm}
Statistical significance is not the only factor to check when looking at the SOM colourings and subgroup results. `r Rpackage('Numero')` can also visualise whether there were districts

1. that had fewer samples matched to them compared to others ([sample hisogram](#qm1)), or 
2. that had a bigger training error compared to the overall map's training error ([matching quality](#qm2)), or
3. that were more than other districts affected by missing values ([coverage](#qm3)).

These are important properties to check before reaching conclusions.

The output of the *nroMatch* function forms the basis for the map quality analysis. Figure 5 shows the pipeline to obtain the quality colourings.

![[Figure 5. ](#fig5) Pipeline for quality colourings](pipeline2.png)

As a pre-requisite, you need the output from *nroMatch*. But then it simply follows the main pipeline using the functions *nroAggregate*, *nroColorize*, *nroLabel*, *nroCircus* and finally *nroFigure* to store the results. There is no need to define a colour range. Except for the map title and the file name, *nroColorize*, *nroLabel*, *nroCircus* and *nroFigure* are used in exactly the same way for all three measures. The only difference is the use of the *nroAggregate* function (Figure 5).

## Sample distribution over the SOM {#qm1}

You can use the `r Rpackage('Numero')` pipeline to visiualise the distribution of individuals/samples over the map. Ideally, individuals/samples should be spread evenly over the map, otherwise some districts of the map will have less power to support your conclusions. For this purpose, do not provide a numeric vector to *nroAggregate* (the *x* parameter), just provide the SOM and the best matching districts. 

```{r quality1, eval=FALSE}
# Visualise sample histogram on the map to verify even distribution.
plane <- nroAggregate(map=sm, bmus=matches$BMU)

# Palette parameter is optional
clrs <- nroColorize(values=plane, palette="fire")
labs <- nroLabel(map=sm, values=plane)
smfig <- nroCircus(map=sm, colors=clrs, labels=labs,
                   title="Data histogram")
nroFigure(file="histogram.svg", scene=smfig)
```

Figure 6 shows that in our example a district aggregated information from between 12 to 18 individuals.

![[Figure 6. ](#fig6) Histogram of sample/individual distribution on the example SOM](histogram.svg)

## Matching quality {#qm2}

To assess the quality of the matching, `r Rpackage('Numero')` compares the overall mean training error to the error for each district. Ideally, the training error should be spread evenly over the map to avoid having one subgroup with a very high training error compared to others.

Technically, the matching quality is computed in the following way: If the overall mean training error is the same as the district's training error the quality measure is 1. If the district's training error is smaller than the mean training error, than the quality measure is smaller than 1, otherwise it is greater than 1. This data is calculated in *nroMatch* and saved in the output column called QUALITY

Consequently, this QUAILTY column can be used as numeric input for *nroAggregate* (*x* parameter) together with the SOM and the best matching districts.

```{r quality2, eval=FALSE}
# Visualise matching quality.
plane <- nroAggregate(map=sm, bmus=matches$BMU, x=matches$QUALITY)

#the palette parameter is optional
clrs <- nroColorize(values=plane, palette="fire")
labs <- nroLabel(map=sm, values=plane)
smfig <- nroCircus(map=sm, colors=clrs, labels=labs,
                   title="Match quality")
nroFigure(file="quality.svg", scene=smfig)
```

Figure 7 shows the quality of our example SOM.

![[Figure 7. ](#fig7) Quality measure for the SOM districts](quality.svg)

Because we used the training samples also for matching, the districts do not show a great divergence from the mean training error.

## Subtype coverage {#qm3}
The coverage map indicates for each district how much data has been available. As different samples are differently affected by missing values, so are the districts. Ideally, 100% (represented by 1.0) of the data has been available but missing values in the dataset might have reduced that number. This visiulation shows whether there are districts that are suffering more than others from missing data.

You will need to feed the RDATA output of the *nroMatch* function together with the SOM and the best matching districts into *nroAggregate*.

```{r quality3, eval=FALSE}
# Visualise the amount of data available per subtype.
plane <- nroAggregate(map=sm, bmus=matches$BMU, x=matches$RDATA)

#the palette parameter is optional
clrs <- nroColorize(values=plane, palette="fire")
labs <- nroLabel(map=sm, values=plane)
smfig <- nroCircus(map=sm, colors=clrs, labels=labs,
                   title="Data coverage")
nroFigure(file="coverage.svg", scene=smfig)
```

As Figure 8 shows that despite some missing values in our dataset, we still achieve good coverage in each district (>= 98%) .

![[Figure 8. ](#fig8) Coverage map for the SOM districts](coverage.svg)

# References



